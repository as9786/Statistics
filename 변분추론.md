# 변분추론(Variational Inference)

## 개념
- 사후확률(posterior) 분포 p(z|x)를 다루기 쉬운 확률분포 q(z)로 근사(approximation)하는 것
- 사후확률 분포를 계산은 힘든 경우가 많음

![image](https://user-images.githubusercontent.com/80622859/212456642-f13a8082-c0f6-4800-a376-67b788776a40.png)

## KL Divergence(Kullback-Leibler divergence, KLD)
- 사후확률에 근사한 q(z)를 만들기 위해 쿨백-라이블러 발산 개념을 활용
- 두 확률분포의 차이를 계산하는데 사용되는 함수
- 사후확률 분포 p(z|x)와 q(z) 사이의 KLD를 계산하고, KLD가 줄어드는 쪽으로 q(z)를 update하는 과정을 반복하면 사후확률을 잘 근사하는 $q^{*}(z)$를 얻게 될 것

- DKL(q(z)||p(z|x))====∫q(z)logq(z)p(z|x)dz∫q(z)logq(z)p(x)p(x|z)p(z)dz∫q(z)logq(z)p(z)dz+∫q(z)logp(x)dz−∫q(z)logp(x|z)dzDKL(q(z)||p(z))+logp(x)−Ez∼q(z)[logp(x|z)]
